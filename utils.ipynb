{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os,pickle,tqdm,json\n",
    "import warnings,json,itertools\n",
    "from collections import Counter\n",
    "import datetime as dt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool,cpu_count\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "def convert_key_tag_top(df):\n",
    "    \n",
    "    cols_to_convert = ['keywords','tags','topics']\n",
    "    \n",
    "    keywords,tags,topics = [],[],[]\n",
    "    for i,v in tqdm.tqdm(df.iterrows()):\n",
    "        if v['keywords']==v['keywords']:\n",
    "            keywords.append((i,list(json.loads(\n",
    "                v['keywords']).keys())))\n",
    "        else:keywords.append((i,[]))\n",
    "        if v['tags']==v['tags']:\n",
    "            tags.append((i,list(json.loads(v['tags']))))\n",
    "        else:tags.append((i,[]))        \n",
    "        if v['topics']==v['topics']:\n",
    "            topics.append((i,list(json.loads(v['topics']))))\n",
    "        else:topics.append((i,[]))\n",
    "\n",
    "    to_join = []\n",
    "    for ls_nm,ls_ls in zip(cols_to_convert,\n",
    "                           [keywords,tags,topics]):\n",
    "        to_join.append(pd.DataFrame(ls_ls).rename(\n",
    "            columns={1:ls_nm}).set_index(0))\n",
    "\n",
    "    df = df.drop(cols_to_convert,axis=1).join(\n",
    "        pd.concat(to_join,axis=1))\n",
    "    \n",
    "    for col in cols_to_convert:\n",
    "        df[f'n_{col}'] = df.apply(\n",
    "            (lambda x:len(x[col])),axis=1)\n",
    "        df[f'1_{col}'] = df.apply(\n",
    "            (lambda x:int(len(x[col])<=1)),axis=1)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def convert_key_tag_top_novelty(df):\n",
    "    \n",
    "    dfs = []\n",
    "    to_remake = ['keywords','topics','tags']\n",
    "    for col in to_remake:\n",
    "        ls = []\n",
    "        for i,v in tqdm.tqdm(df.iterrows()):\n",
    "            if v[col]==v[col]:\n",
    "                ls.append((i,sorted(json.loads(\n",
    "                    v[col].replace(\"\\'\",'\\\"')))))\n",
    "            else:ls.append((i,[]))\n",
    "        df_ = pd.DataFrame(ls).rename(columns={1:col}).set_index(0)\n",
    "        dfs.append(df_)\n",
    "\n",
    "    dfs = pd.concat(dfs,axis=1)\n",
    "    df = df.drop(to_remake,axis=1).join(dfs)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def flatten_list_to_set(ls_of_ls):\n",
    "    return set([it for sl in ls_of_ls for it in sl])\n",
    "\n",
    "def plot_weighted_circular(G,weighted=True,\n",
    "                           plot=True,community=None):\n",
    "    \n",
    "    if plot==True:\n",
    "        plt.figure(figsize=(5,5))\n",
    "        pos = nx.circular_layout(G)\n",
    "        nx.draw_networkx_nodes(G,pos,node_size=20)\n",
    "        \n",
    "        if weighted:\n",
    "            \n",
    "            width = np.array(\n",
    "                [info[2]['weight'] for\\\n",
    "                 info in G.edges(data=True)])\n",
    "            \n",
    "            if max(width)>10:\n",
    "                width=width/10.0\n",
    "                \n",
    "            if community:\n",
    "                \n",
    "                edge_list = [\n",
    "                    i for i,v in same_diff_comm.items() if v==True]\n",
    "                nx.draw_networkx_edges(\n",
    "                    G,pos,edge_list,width=width,alpha=0.25)\n",
    "                \n",
    "                edge_list = [\n",
    "                    i for i,v in same_diff_comm.items() if v==False]\n",
    "                nx.draw_networkx_edges(\n",
    "                    G,pos,edge_list,edge_color='r',\n",
    "                    width=width,alpha=0.25)  \n",
    "                \n",
    "            else:\n",
    "                nx.draw_networkx_edges(G,pos,width=width,alpha=0.5)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            edge_list = [\n",
    "                i for i,v in same_diff_comm.items() if v==True]\n",
    "            nx.draw_networkx_edges(\n",
    "                G,pos,edge_list,alpha=0.25)\n",
    "            \n",
    "            edge_list = [\n",
    "                i for i,v in same_diff_comm.items() if v==False]\n",
    "            nx.draw_networkx_edges(\n",
    "                G,pos,edge_list,\n",
    "                edge_color='r',alpha=0.25) \n",
    "            \n",
    "        plt.show()\n",
    "        \n",
    "def get_pairwise_combo(ls,self_loop=True):\n",
    "    \n",
    "    if len(ls)>=1:\n",
    "        combo = list(itertools.combinations(ls,2))\n",
    "        combo = list(tuple(sorted(i)) for i in combo)\n",
    "        \n",
    "        if self_loop:\n",
    "            for it in ls:\n",
    "                combo.append((it,it))\n",
    "            \n",
    "    else:\n",
    "        combo = []\n",
    "    \n",
    "    return combo\n",
    "        \n",
    "def get_observed_freq(df,col='topics',self_loop=False):\n",
    "    \n",
    "    nodes = []\n",
    "    for sublist in df[col].values:\n",
    "        for item in sublist:\n",
    "            nodes.append(item)\n",
    "    all_edges = get_pairwise_combo(nodes,self_loop)\n",
    "    \n",
    "    obs_edges = []\n",
    "    for i,v in df.iterrows():\n",
    "        \n",
    "        if len(v[col])==1:\n",
    "            combo = get_pairwise_combo(v[col],self_loop)\n",
    "        else:\n",
    "            combo = get_pairwise_combo(v[col],False)\n",
    "            \n",
    "        if combo:\n",
    "            for pair in combo:\n",
    "                pair = tuple(sorted(pair))\n",
    "                obs_edges.append(pair)\n",
    "    obs_edges = dict(Counter(obs_edges))\n",
    "\n",
    "    final_counts = {edge:0 for edge in all_edges}\n",
    "    for edge,weight in obs_edges.items():\n",
    "        final_counts[edge] = weight\n",
    "        \n",
    "    return final_counts\n",
    "\n",
    "def get_expected_freq(\n",
    "    observed_freq,plot=True,self_loop=False):\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    for edge,weight in observed_freq.items():\n",
    "        if weight>0:\n",
    "            G.add_edge(edge[0],edge[1],weight=weight)\n",
    "\n",
    "    if plot:\n",
    "        print('number of edges:',len(G.edges(data=True)))\n",
    "        plot_weighted_circular(G) \n",
    "        \n",
    "    try:\n",
    "        swaps = nx.connected_double_edge_swap(G,10)\n",
    "    except:\n",
    "        swaps = nx.double_edge_swap(G)\n",
    "        \n",
    "    if plot: \n",
    "        print('number of swaps:',swaps)\n",
    "        print('randomly assign edge weights')\n",
    "        \n",
    "    weights = [w for w in list(\n",
    "        observed_freq.values()) if w>0]\n",
    "    \n",
    "    for edge in G.edges(data=True):\n",
    "        chosen = np.random.choice(weights)\n",
    "        weights.remove(chosen)\n",
    "        G[edge[0]][edge[1]]['weight'] = chosen\n",
    "    \n",
    "    if plot:\n",
    "        print('number of edges:',len(G.edges(data=True)))\n",
    "        plot_weighted_circular(G)\n",
    "\n",
    "    expected_freq = {tuple(sorted(i)):0 for\\\n",
    "                           i,_ in observed_freq.items()}\n",
    "    for u,v,w in G.edges(data=True):\n",
    "        expected_freq[tuple(sorted((u,v)))]=w['weight']\n",
    "        \n",
    "    return expected_freq\n",
    "\n",
    "def uzzi2013(posts,measure_col='topics',interval='m',datetime_col='datetime'):\n",
    "\n",
    "    # first convert the datetime to the time interval we want\n",
    "    # then we can look at the subgraph for each time interval\n",
    "    try:\n",
    "        posts[datetime_col] = pd.to_datetime(\n",
    "            posts[datetime_col]).dt.to_period(interval)\n",
    "    except: pass\n",
    "    months = sorted(posts[datetime_col].value_counts().keys())\n",
    "    n_posts_too_few_in_period = 0\n",
    "    n_no_z_scores = 0\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print(f'no. of cores found: {cpu_count()}\\n')\n",
    "    print(f'relying on 1 process')\n",
    "\n",
    "    # we look at subgraphs by periods\n",
    "    # here we use month but we can use days\n",
    "    # depending on the dataset\n",
    "    for count,month in enumerate(months):\n",
    "        \n",
    "        period = posts[posts[datetime_col]==month]\n",
    "        observed_freq = get_observed_freq(\n",
    "            period,measure_col,True)\n",
    "        all_topics = set(\n",
    "            [it for sl in period[measure_col] for it in sl])\n",
    "\n",
    "        # if during the time period there is less than 2 post\n",
    "        # of if during the time period there are less than 5 topics\n",
    "        # which means that there are too few nodes\n",
    "        if (len(all_topics)>=4) & (len(period)>=2):\n",
    "\n",
    "            for idx,val in tqdm.tqdm(\n",
    "                period.iterrows(),\n",
    "                desc=f'{count+1}/{len(months)}'):\n",
    "\n",
    "                z_score = {}\n",
    "                z_score['observed'] = {}\n",
    "                combos = get_pairwise_combo(val[measure_col])\n",
    "\n",
    "                for combo in combos:\n",
    "                    z_score['observed'][combo] = observed_freq[combo]\n",
    "\n",
    "                for sim in range(20):\n",
    "                    expected_freq = get_expected_freq(\n",
    "                        observed_freq,False,True)\n",
    "\n",
    "                    z_score[f'expected_{sim}'] = {}\n",
    "                    for combo in get_pairwise_combo(val[measure_col]):                                            \n",
    "                        z_score[f'expected_{sim}'][combo] = \\\n",
    "                        expected_freq[combo]\n",
    "\n",
    "                z_score = pd.DataFrame.from_dict(z_score)   \n",
    "\n",
    "                obs = z_score['observed']\n",
    "                exp = z_score.drop('observed',axis=1).mean(axis=1)\n",
    "                std = z_score.drop('observed',axis=1).std(axis=1)\n",
    "                z = ((obs-exp)/(std)).values\n",
    "                z = [val for val in z if val==val]\n",
    "\n",
    "                if z:\n",
    "                    posts.loc[idx,'novelty_median'] = np.nanmedian(z)\n",
    "                    posts.loc[idx,'novelty_tenth'] = np.nanpercentile(z,10)\n",
    "                else:\n",
    "                    n_no_z_scores+=1\n",
    "        else:\n",
    "            n_posts_too_few_in_period+=1\n",
    "            \n",
    "    return posts\n",
    "\n",
    "def foster2015(posts,measure_col='topics',\n",
    "               interval='m',detection_method=None,\n",
    "               datetime_col='datetime'):\n",
    "    try:\n",
    "        posts[datetime_col] = pd.to_datetime(\n",
    "            posts[datetime_col]).dt.to_period(interval)\n",
    "    except:pass\n",
    "    months = sorted(posts[datetime_col].value_counts().keys())\n",
    "    n_posts_too_few_in_period = 0\n",
    "    n_no_z_scores = 0\n",
    "\n",
    "    # we look at subgraphs by periods\n",
    "    # here we use month but we can use days\n",
    "    # depending on the dataset\n",
    "    for count,month in enumerate(months):\n",
    "        \n",
    "        period = posts[posts[datetime_col]==month]\n",
    "        observed_freq = get_observed_freq(\n",
    "            period,measure_col,True)\n",
    "        all_topics = set(\n",
    "            [it for sl in period[measure_col] for it in sl])\n",
    "        G = nx.Graph()\n",
    "\n",
    "        for (u,v),w in observed_freq.items():\n",
    "            G.add_edge(u,v,weight=w)\n",
    "\n",
    "        if detection_method=='louvian':\n",
    "            communities = nx.algorithms.community.louvain_communities(\n",
    "                G,weight='weight')\n",
    "        else:\n",
    "            communities = nx.algorithms.community.greedy_modularity_communities(\n",
    "                G,weight='weight')\n",
    "        communities = {it:n for n,ls in enumerate(communities) for it in ls}\n",
    "        same_diff_comm = {tuple(sorted((u,v))):\\\n",
    "                          communities[u]==communities[v] for u,v in G.edges()}\n",
    "\n",
    "        if (len(all_topics)>=4) & (len(period)>=2):\n",
    "\n",
    "            for idx,val in tqdm.tqdm(\n",
    "                period.iterrows(),\n",
    "                desc=f'{count+1}/{len(months)}'):\n",
    "                combos = get_pairwise_combo(val[measure_col])\n",
    "                counts = len(combos)\n",
    "                # note that True means same community False means diff community\n",
    "                scores = [0 if same_diff_comm[combo] else 1 for combo in combos]\n",
    "                novelty = np.sum(scores)/counts\n",
    "                posts.loc[idx,'novelty_foster'] = novelty\n",
    "                \n",
    "    return posts\n",
    "\n",
    "def lee2015(posts,measure_col='topics',\n",
    "            interval='m',\n",
    "            datetime_col='datetime'):\n",
    "    \n",
    "    try:\n",
    "        posts[datetime_col] = pd.to_datetime(\n",
    "            posts[datetime_col]).dt.to_period(interval)\n",
    "    except:pass\n",
    "    \n",
    "    months = sorted(posts[datetime_col].value_counts().keys())\n",
    "    n_posts_too_few_in_period = 0\n",
    "    n_no_z_scores = 0\n",
    "\n",
    "    # we look at subgraphs by periods\n",
    "    # here we use month but we can use days\n",
    "    # depending on the dataset\n",
    "    for count,month in enumerate(months):\n",
    "        \n",
    "        period = posts[posts[datetime_col]==month]\n",
    "        observed_freq = get_observed_freq(\n",
    "            period,measure_col,True); print('get observed freq...')\n",
    "        all_topics = set(\n",
    "            [it for sl in period[measure_col] for it in sl])\n",
    "        Nt = len(observed_freq); print('get Nt...')\n",
    "        Nit = len(all_topics)+1; print('get Nit...')\n",
    "        Njt = len(all_topics)+1; print('get Njt...')\n",
    "        denominator = Nit*Njt; print('get denominator...\\n')\n",
    "\n",
    "        for idx,val in tqdm.tqdm(period.iterrows(),\n",
    "                                 desc=f'{count+1}/{len(months)}'):\n",
    "\n",
    "            combos = get_pairwise_combo(val[measure_col])\n",
    "            commonness = []\n",
    "            for combo in combos:\n",
    "                numerator = observed_freq[combo]*Nt\n",
    "                commonness.append(numerator/denominator)  \n",
    "\n",
    "            commonness = [c for c in commonness if c>0]\n",
    "            commonness = np.percentile(commonness,10)\n",
    "            posts.loc[idx,'commonness_lee'] = commonness\n",
    "\n",
    "    return posts\n",
    "\n",
    "def _uzzi2013_parallel_1_period(item):\n",
    "    \n",
    "    count,period,measure_col = item\n",
    "    observed_freq = get_observed_freq(\n",
    "        period,measure_col,True)\n",
    "    all_topics = set(\n",
    "        [it for sl in period[measure_col] for it in sl])\n",
    "\n",
    "    # if during the time period there is less than 2 post\n",
    "    # of if during the time period there are less than 5 topics\n",
    "    # which means that there are too few nodes\n",
    "    \n",
    "    if (len(all_topics)>=4) & (len(period)>=2):\n",
    "\n",
    "        for idx,val in tqdm.tqdm(period.iterrows()):\n",
    "\n",
    "            z_score = {}\n",
    "            z_score['observed'] = {}\n",
    "            combos = get_pairwise_combo(val[measure_col])\n",
    "\n",
    "            for combo in combos:\n",
    "                z_score['observed'][combo] = observed_freq[combo]\n",
    "\n",
    "            for sim in range(20):\n",
    "                expected_freq = get_expected_freq(\n",
    "                    observed_freq,False,True)\n",
    "\n",
    "                z_score[f'expected_{sim}'] = {}\n",
    "                for combo in get_pairwise_combo(val[measure_col]):                                            \n",
    "                    z_score[f'expected_{sim}'][combo] = \\\n",
    "                    expected_freq[combo]\n",
    "\n",
    "            z_score = pd.DataFrame.from_dict(z_score)   \n",
    "            obs = z_score['observed']\n",
    "            exp = z_score.drop('observed',axis=1).mean(axis=1)\n",
    "            std = z_score.drop('observed',axis=1).std(axis=1)\n",
    "            z = ((obs-exp)/(std)).values\n",
    "            z = [val for val in z if val==val]\n",
    "\n",
    "            if z:\n",
    "                period.loc[idx,'novelty_median'] = np.nanmedian(z)\n",
    "                period.loc[idx,'novelty_tenth'] = np.nanpercentile(z,10)\n",
    "            else:\n",
    "                period.loc[idx,'novelty_median'] = np.NaN\n",
    "                period.loc[idx,'novelty_tenth'] = np.NaN\n",
    "    \n",
    "    else:\n",
    "        period.loc[:,'novelty_median'] = np.NaN\n",
    "        period.loc[:,'novelty_tenth'] = np.NaN\n",
    "        \n",
    "    return period\n",
    "\n",
    "def uzzi2013_parallel_1(\n",
    "    posts,measure_col='topics',interval='m',\n",
    "    datetime_col='datetime'):\n",
    "    \n",
    "    # first convert the datetime to the time interval we want\n",
    "    # then we can look at the subgraph for each time interval\n",
    "    try:\n",
    "        posts[datetime_col] = pd.to_datetime(\n",
    "            posts[datetime_col]).dt.to_period(interval)\n",
    "    except: pass\n",
    "\n",
    "    months = sorted(posts[datetime_col].value_counts().keys())\n",
    "    n_posts_too_few_in_period = 0\n",
    "    n_no_z_scores = 0\n",
    "    iteration = [(\n",
    "        count,posts[posts[datetime_col]==month],\n",
    "        measure_col) for count,month in enumerate(months)]\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print(f'no. of cores found: {cpu_count()}\\n')\n",
    "    n_processes = min(len(iteration),cpu_count())\n",
    "    print(f'relying on {n_processes} processes')\n",
    "    pool = Pool(processes=n_processes)\n",
    "    results = list(tqdm.tqdm(pool.map(\n",
    "        _uzzi2013_parallel_1_period,iteration)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return pd.concat(results)\n",
    "\n",
    "def _uzzi2013_parallel_2_row(item):\n",
    "    \n",
    "    observed_freq,measure_col,idx,val = item\n",
    "    \n",
    "    z_score = {}\n",
    "    z_score['observed'] = {}\n",
    "    combos = get_pairwise_combo(val[measure_col])\n",
    "\n",
    "    for combo in combos:\n",
    "        z_score['observed'][combo] = observed_freq[combo]\n",
    "\n",
    "    for sim in range(20):\n",
    "        expected_freq = get_expected_freq(\n",
    "            observed_freq,False,True)\n",
    "\n",
    "        z_score[f'expected_{sim}'] = {}\n",
    "        for combo in get_pairwise_combo(val[measure_col]):                                            \n",
    "            z_score[f'expected_{sim}'][combo] = \\\n",
    "            expected_freq[combo]\n",
    "\n",
    "    z_score = pd.DataFrame.from_dict(z_score)   \n",
    "\n",
    "    obs = z_score['observed']\n",
    "    exp = z_score.drop('observed',axis=1).mean(axis=1)\n",
    "    std = z_score.drop('observed',axis=1).std(axis=1)\n",
    "    z = ((obs-exp)/(std)).values\n",
    "    z = [val for val in z if val==val]\n",
    "\n",
    "    if z:\n",
    "        return (idx,np.nanmedian(z),np.nanpercentile(z,10))\n",
    "    else:\n",
    "        return (idx,np.NaN,np.NaN)\n",
    "    \n",
    "def uzzi2013_parallel_2(\n",
    "    posts,measure_col='topics',interval='m',\n",
    "    datetime_col='datetime'):\n",
    "\n",
    "    # first convert the datetime to the time interval we want\n",
    "    # then we can look at the subgraph for each time interval\n",
    "    try:\n",
    "        posts[datetime_col] = pd.to_datetime(\n",
    "            posts[datetime_col]).dt.to_period(interval)\n",
    "    except: pass\n",
    "    \n",
    "    months = sorted(posts[datetime_col].value_counts().keys())\n",
    "    z_scores = []\n",
    "\n",
    "    # we look at subgraphs by periods\n",
    "    # here we use month but we can use days\n",
    "    # depending on the dataset\n",
    "    for count,month in enumerate(months):\n",
    "        \n",
    "        period = posts[posts[datetime_col]==month]\n",
    "        observed_freq = get_observed_freq(\n",
    "            period,measure_col,True)\n",
    "        all_topics = set(\n",
    "            [it for sl in period[measure_col] for it in sl])\n",
    "\n",
    "        # if during the time period there is less than 2 post\n",
    "        # of if during the time period there are less than 5 topics\n",
    "        # which means that there are too few nodes\n",
    "        if (len(all_topics)>=4) & (len(period)>=2):\n",
    "\n",
    "            iteration = [(observed_freq,measure_col,idx,val) for\\\n",
    "                         idx,val in period.iterrows()]\n",
    "            print('\\n\\n')\n",
    "            print(f'no. of cores found: {cpu_count()}\\n')\n",
    "            n_processes = min(len(iteration),cpu_count())\n",
    "            print(f'relying on {n_processes} processes')\n",
    "            pool = Pool(processes=n_processes)\n",
    "            results = list(tqdm.tqdm(pool.map(_uzzi2013_parallel_2_row,iteration,100)))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            z_scores+=results\n",
    "            \n",
    "    z_scores = pd.DataFrame(z_scores,columns=[\n",
    "        'index','novelty_median','novelty_tenth'])\n",
    "            \n",
    "    return posts.join(z_scores.set_index('index'))\n",
    "\n",
    "def get_novelty(df,measure_col='topics',interval='m',parallel=1):\n",
    "\n",
    "    start_time = dt.datetime.now()\n",
    "    \n",
    "    if parallel==0:\n",
    "        df = uzzi2013(\n",
    "            df,\n",
    "            measure_col=measure_col,\n",
    "            interval=interval)\n",
    "    elif parallel==1:\n",
    "        df = uzzi2013_parallel_1(\n",
    "            df,\n",
    "            measure_col=measure_col,\n",
    "            interval=interval)\n",
    "    else:\n",
    "        df = uzzi2013_parallel_2(\n",
    "            df,\n",
    "            measure_col=measure_col,\n",
    "            interval=interval)\n",
    "        \n",
    "    print(f\"total processing time:\",\n",
    "          dt.datetime.now() - start_time)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2500it [00:00, 18793.33it/s]\n",
      "2500it [00:00, 21996.05it/s]\n",
      "2500it [00:00, 25107.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "no. of cores found: 52\n",
      "\n",
      "relying on 1 process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/6: 19it [00:00, 124.57it/s]\n",
      "6/6: 2475it [03:03, 13.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total processing time: 0:03:06.765137\n",
      "\n",
      "\n",
      "\n",
      "no. of cores found: 52\n",
      "\n",
      "relying on 6 processes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "19it [00:00, 132.51it/s]\n",
      "2475it [02:40, 15.41it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 32430.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total processing time: 0:02:44.212025\n",
      "\n",
      "\n",
      "\n",
      "no. of cores found: 52\n",
      "\n",
      "relying on 19 processes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 19/19 [00:00<00:00, 106539.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "no. of cores found: 52\n",
      "\n",
      "relying on 52 processes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2475/2475 [00:00<00:00, 1311713.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total processing time: 0:00:35.189069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "posts = convert_key_tag_top_novelty(pd.read_csv('./data/xhs/users_clean.csv',\n",
    "                                                index_col='index'),'crawl_date')\n",
    "\n",
    "posts_0 = posts.copy()\n",
    "posts_1 = posts.copy()\n",
    "posts_2 = posts.copy()\n",
    "\n",
    "posts_0_res = get_novelty(posts_0,parallel=0)\n",
    "posts_1_res = get_novelty(posts_1,parallel=1)\n",
    "posts_2_res = get_novelty(posts_2,parallel=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "novelty_cols = ['novelty_median','novelty_tenth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>novelty_median</th>\n",
       "      <th>novelty_tenth</th>\n",
       "      <th>novelty_median</th>\n",
       "      <th>novelty_tenth</th>\n",
       "      <th>novelty_median</th>\n",
       "      <th>novelty_tenth</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5aa6791a4eacab6fb05bc81f</th>\n",
       "      <td>4.239885</td>\n",
       "      <td>0.736751</td>\n",
       "      <td>1.019479</td>\n",
       "      <td>0.604474</td>\n",
       "      <td>1.019479</td>\n",
       "      <td>0.604474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e5a296d0000000001001efa</th>\n",
       "      <td>0.531156</td>\n",
       "      <td>0.531156</td>\n",
       "      <td>0.810701</td>\n",
       "      <td>0.810701</td>\n",
       "      <td>0.810701</td>\n",
       "      <td>0.810701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5bf81a8adb2e604af5301e57</th>\n",
       "      <td>0.674944</td>\n",
       "      <td>0.674944</td>\n",
       "      <td>3.846498</td>\n",
       "      <td>3.846498</td>\n",
       "      <td>3.849591</td>\n",
       "      <td>3.849591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5ebeb442000000000101f543</th>\n",
       "      <td>5.913032</td>\n",
       "      <td>0.046471</td>\n",
       "      <td>3.337863</td>\n",
       "      <td>-0.197189</td>\n",
       "      <td>3.344355</td>\n",
       "      <td>-0.236372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5659bc677c5bb86279284626</th>\n",
       "      <td>1.919322</td>\n",
       "      <td>-0.140343</td>\n",
       "      <td>1.131582</td>\n",
       "      <td>0.421613</td>\n",
       "      <td>1.137764</td>\n",
       "      <td>0.380112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59d87fcd51783a781df21578</th>\n",
       "      <td>17.292169</td>\n",
       "      <td>17.292169</td>\n",
       "      <td>15.181288</td>\n",
       "      <td>15.181288</td>\n",
       "      <td>14.038235</td>\n",
       "      <td>14.038235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5ef0c8fc0000000001000fe7</th>\n",
       "      <td>11.133020</td>\n",
       "      <td>11.133020</td>\n",
       "      <td>3.055492</td>\n",
       "      <td>3.055492</td>\n",
       "      <td>21.569256</td>\n",
       "      <td>21.569256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5d2b5955000000001100c778</th>\n",
       "      <td>66.416464</td>\n",
       "      <td>66.416464</td>\n",
       "      <td>18.345776</td>\n",
       "      <td>18.345776</td>\n",
       "      <td>140.698783</td>\n",
       "      <td>140.698783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5b41fa9211be104a1dd6cf77</th>\n",
       "      <td>1.887051</td>\n",
       "      <td>1.887051</td>\n",
       "      <td>-0.002775</td>\n",
       "      <td>-0.002775</td>\n",
       "      <td>0.050157</td>\n",
       "      <td>0.050157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e4e832c0000000001001128</th>\n",
       "      <td>0.008901</td>\n",
       "      <td>-0.069191</td>\n",
       "      <td>0.296434</td>\n",
       "      <td>0.258495</td>\n",
       "      <td>0.054609</td>\n",
       "      <td>-0.048202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          novelty_median  novelty_tenth  novelty_median  \\\n",
       "index                                                                     \n",
       "5aa6791a4eacab6fb05bc81f        4.239885       0.736751        1.019479   \n",
       "5e5a296d0000000001001efa        0.531156       0.531156        0.810701   \n",
       "5bf81a8adb2e604af5301e57        0.674944       0.674944        3.846498   \n",
       "5ebeb442000000000101f543        5.913032       0.046471        3.337863   \n",
       "5659bc677c5bb86279284626        1.919322      -0.140343        1.131582   \n",
       "...                                  ...            ...             ...   \n",
       "59d87fcd51783a781df21578       17.292169      17.292169       15.181288   \n",
       "5ef0c8fc0000000001000fe7       11.133020      11.133020        3.055492   \n",
       "5d2b5955000000001100c778       66.416464      66.416464       18.345776   \n",
       "5b41fa9211be104a1dd6cf77        1.887051       1.887051       -0.002775   \n",
       "5e4e832c0000000001001128        0.008901      -0.069191        0.296434   \n",
       "\n",
       "                          novelty_tenth  novelty_median  novelty_tenth  \n",
       "index                                                                   \n",
       "5aa6791a4eacab6fb05bc81f       0.604474        1.019479       0.604474  \n",
       "5e5a296d0000000001001efa       0.810701        0.810701       0.810701  \n",
       "5bf81a8adb2e604af5301e57       3.846498        3.849591       3.849591  \n",
       "5ebeb442000000000101f543      -0.197189        3.344355      -0.236372  \n",
       "5659bc677c5bb86279284626       0.421613        1.137764       0.380112  \n",
       "...                                 ...             ...            ...  \n",
       "59d87fcd51783a781df21578      15.181288       14.038235      14.038235  \n",
       "5ef0c8fc0000000001000fe7       3.055492       21.569256      21.569256  \n",
       "5d2b5955000000001100c778      18.345776      140.698783     140.698783  \n",
       "5b41fa9211be104a1dd6cf77      -0.002775        0.050157       0.050157  \n",
       "5e4e832c0000000001001128       0.258495        0.054609      -0.048202  \n",
       "\n",
       "[2500 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([posts_0_res[novelty_cols],posts_1_res[novelty_cols],posts_2_res[novelty_cols]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>novelty_median</th>\n",
       "      <th>novelty_tenth</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5aa6791a4eacab6fb05bc81f</th>\n",
       "      <td>4.239885</td>\n",
       "      <td>0.736751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e5a296d0000000001001efa</th>\n",
       "      <td>0.531156</td>\n",
       "      <td>0.531156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5bf81a8adb2e604af5301e57</th>\n",
       "      <td>0.674944</td>\n",
       "      <td>0.674944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5ebeb442000000000101f543</th>\n",
       "      <td>5.913032</td>\n",
       "      <td>0.046471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5659bc677c5bb86279284626</th>\n",
       "      <td>1.919322</td>\n",
       "      <td>-0.140343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59d87fcd51783a781df21578</th>\n",
       "      <td>17.292169</td>\n",
       "      <td>17.292169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5ef0c8fc0000000001000fe7</th>\n",
       "      <td>11.133020</td>\n",
       "      <td>11.133020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5d2b5955000000001100c778</th>\n",
       "      <td>66.416464</td>\n",
       "      <td>66.416464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5b41fa9211be104a1dd6cf77</th>\n",
       "      <td>1.887051</td>\n",
       "      <td>1.887051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e4e832c0000000001001128</th>\n",
       "      <td>0.008901</td>\n",
       "      <td>-0.069191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          novelty_median  novelty_tenth\n",
       "index                                                  \n",
       "5aa6791a4eacab6fb05bc81f        4.239885       0.736751\n",
       "5e5a296d0000000001001efa        0.531156       0.531156\n",
       "5bf81a8adb2e604af5301e57        0.674944       0.674944\n",
       "5ebeb442000000000101f543        5.913032       0.046471\n",
       "5659bc677c5bb86279284626        1.919322      -0.140343\n",
       "...                                  ...            ...\n",
       "59d87fcd51783a781df21578       17.292169      17.292169\n",
       "5ef0c8fc0000000001000fe7       11.133020      11.133020\n",
       "5d2b5955000000001100c778       66.416464      66.416464\n",
       "5b41fa9211be104a1dd6cf77        1.887051       1.887051\n",
       "5e4e832c0000000001001128        0.008901      -0.069191\n",
       "\n",
       "[2500 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_0_res[novelty_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_138002/3135121019.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mposts_1_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnovelty_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mposts_2_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnovelty_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
     ]
    }
   ],
   "source": [
    "posts_1_res[novelty_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_2_res[novelty_cols]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video_env_1",
   "language": "python",
   "name": "video_env_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
