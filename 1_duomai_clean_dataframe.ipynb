{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55726/117258227.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwarnings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import os,tqdm,warnings,json\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import convert_key_tag_top\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and clean user dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_excel('./data/xhs/users.xlsx',\n",
    "                      index_col='Creator_ID',\n",
    "                      parse_dates=['Crawl_Date','insert_time'])\n",
    "users = users.replace(\n",
    "    {'腰部':1, '超头部':4, '头部':3, '尾部':0, '肩部':2})\n",
    "users = users.rename(columns={'insert_time':'post_date',\n",
    "                              'Creator_Type':'topics',\n",
    "                              'collect':'Collect_Count',\n",
    "                              'Creator_Like_Count':'Like_Count',\n",
    "                              'Creator_Post_Counts':'Post_Count',\n",
    "                              'Creator_Fan_Count':'Fan_Count'})\n",
    "users = users[['post_date','Crawl_Date',\n",
    "               'Gender','Post_Count','Fan_Count',\n",
    "               'Like_Count','Collect_Count',\n",
    "               'topics','tags','keywords',\n",
    "               'kol_level']]\n",
    "users = convert_key_tag_top(users)\n",
    "users.index.name = 'index'\n",
    "users.columns = [col.lower() for col in users.columns]\n",
    "users.to_csv('./data/xhs/users_clean.csv')\n",
    "print('dumped to: ./data/xhs/users_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(4,1,figsize=(5,5))\n",
    "for n,i in enumerate([i for i in users.columns if 'count' in i]):\n",
    "    ax[n].hist(users[i].values,bins=100,log=True)\n",
    "    ax[n].set_title(i+' (log)',fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot more distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean of creators:')\n",
    "f,ax = plt.subplots(3,1,figsize=(5,5))\n",
    "for n,i in enumerate([i for i in users.columns if i.startswith('n_')]):\n",
    "    ax[n].hist(users[i].values,bins=10)\n",
    "    ax[n].set_title(i,fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and clean post dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []\n",
    "for i in tqdm.tqdm(os.listdir('./data/xhs/posts/')):\n",
    "    if i.endswith('.xlsx'):\n",
    "        posts.append(pd.read_excel('./data/xhs/posts/'+i,\n",
    "                                   index_col='post_id'))\n",
    "\n",
    "posts = pd.concat(posts)\n",
    "posts['if_video'] = posts.apply((\n",
    "    lambda x:1 if x['post_type']=='video' else 0),axis=1)\n",
    "posts['if_cooperate'] = posts.apply((\n",
    "    lambda x:1 if x['cooperate']==x['cooperate'] else 0),axis=1)\n",
    "posts = posts.drop(['post_content','barrage','coin',\n",
    "                    'barrages','barrage_keywords',\n",
    "                    'cooperate'],axis=1)\n",
    "posts = posts.rename(columns={\n",
    "    'ai_pred_topics':'topics','tag_ids':'tags'})\n",
    "posts = convert_key_tag_top(posts)\n",
    "posts.index.name = 'index'\n",
    "posts.columns = [col.lower() for col in posts.columns]\n",
    "posts.to_csv('./data/xhs/posts_clean.csv')\n",
    "print('dumped to: ./data/xhs/posts_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot post distibution by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = posts.reset_index().set_index('user_id').join(users).set_index('post_id')\n",
    "posts['post_date'] = pd.to_datetime(posts['post_date']).dt.to_period('d')\n",
    "posts['crawl_date'] = pd.to_datetime(posts['crawl_date']).dt.to_period('d')\n",
    "f,axes = plt.subplots(2,1,figsize=(8,4))\n",
    "for n,col in enumerate(['post_date','crawl_date']):\n",
    "    X,Y = [],[]\n",
    "    for x,y in posts[[col]].value_counts().sort_index().items():\n",
    "        X.append(str(x[0]))\n",
    "        Y.append(y)\n",
    "    \n",
    "    axes[n].plot(X,Y)\n",
    "    axes[n].set_title(col)\n",
    "    axes[n].set_ylabel('No. of posts per day')\n",
    "    axes[n].set_xticks(np.arange(0,len(X),len(X)//10))\n",
    "    axes[n].set_xticklabels([X[i] for i in range(0,len(X),len(X)//10)],\n",
    "                            rotation=15,va='center',position=(0,-0.08))\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'posts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_103593/256577113.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m print('Average no. of keywords per post:',\n\u001b[0;32m----> 2\u001b[0;31m       int(posts['n_keywords'].mean()))\n\u001b[0m\u001b[1;32m      3\u001b[0m print('Average no. of tags per post:',\n\u001b[1;32m      4\u001b[0m       int(posts['n_tags'].mean()))\n\u001b[1;32m      5\u001b[0m print('Average no. of topics per post:',\n",
      "\u001b[0;31mNameError\u001b[0m: name 'posts' is not defined"
     ]
    }
   ],
   "source": [
    "print('Average no. of keywords per post:',\n",
    "      int(posts['n_keywords'].mean()))\n",
    "print('Average no. of tags per post:',\n",
    "      int(posts['n_tags'].mean()))\n",
    "print('Average no. of topics per post:',\n",
    "      int(posts['n_topics'].mean()))\n",
    "print()\n",
    "print('Percentage of posts with <= 1 keyword:',\n",
    "      int(100*(posts['1_keywords'].sum()/len(posts))),'%')\n",
    "print('Percentage of posts with <= 1 tag:',\n",
    "      int(100*(posts['1_tags'].sum()/len(posts))),'%')\n",
    "print('Percentage of posts with <= 1 topic:',\n",
    "      int(100*(posts['1_topics'].sum()/len(posts))),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Posts with =1 topic:',posts[posts['n_topics']==1][\n",
    "    'post_like'].mean(),'Average likes')\n",
    "print('Posts with >1 topic:',posts[posts['n_topics']>=1][\n",
    "    'post_like'].mean(),'Average likes')\n",
    "print()\n",
    "print('Posts with =1 topic:',posts[posts['n_topics']==1][\n",
    "    'collect'].mean(),'Average collects')\n",
    "print('Posts with >1 topic:',posts[posts['n_topics']>=1][\n",
    "    'collect'].mean(),'Average collects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = sorted(posts['post_date'].value_counts().keys())\n",
    "for count,month in enumerate(months):\n",
    "    period = posts[posts['post_date']==month]\n",
    "    for idx,val in tqdm.tqdm(period.iterrows(),\n",
    "                             desc=f'{count+1}/{len(months)}'):   \n",
    "        posts.loc[idx,'post_like_normed'] = val[\n",
    "            'post_like']/period['post_like'].sum()\n",
    "        posts.loc[idx,'collect_normed'] = val[\n",
    "            'collect']/period['collect'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_sig(df=None):\n",
    "    \n",
    "    p_matrix = np.zeros(shape=(df.shape[1],df.shape[1]))\n",
    "    for col in df.columns:\n",
    "        for col2 in df.drop(col,axis=1).columns:\n",
    "            _ , p = stats.pearsonr(df[col],df[col2])\n",
    "            p_matrix[df.columns.to_list().index(col),\n",
    "                     df.columns.to_list().index(col2)] = p\n",
    "    p_matrix = pd.DataFrame(p_matrix)\n",
    "    to_rename = dict(enumerate(df.columns.values))\n",
    "    \n",
    "    return p_matrix.rename(index=to_rename,columns=to_rename)\n",
    "\n",
    "def plot_cor_matrix(corr,mask=None,labels=None):\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(11,9))\n",
    "    sns.heatmap(corr,ax=ax,\n",
    "                mask=mask,\n",
    "                annot=True,\n",
    "                vmin=-1,vmax=1,\n",
    "                center=0,\n",
    "                cmap='coolwarm',\n",
    "                linewidths=2, \n",
    "                square=True,\n",
    "                linecolor='black', \n",
    "                cbar_kws={'orientation':'vertical'})\n",
    "    \n",
    "df = posts[['post_like','collect',\n",
    "            'post_like_normed','collect_normed',\n",
    "            'n_topics','n_tags','n_keywords',\n",
    "            '1_topics','1_tags','1_keywords',]]\n",
    "df = df.dropna(how='any').copy()\n",
    "corr = df.corr()\n",
    "pval = corr_sig(df)\n",
    "corr_label = corr.copy()\n",
    "for idx_row,row_val in corr.iterrows():\n",
    "    for idx_col,col_val in row_val.items():\n",
    "        pval_item = pval.loc[idx_row,idx_col]\n",
    "        corr_label.loc[idx_row,idx_col] = stars(col_val,pval_item,2)\n",
    "corr_label = corr_label.values.tolist()\n",
    "mask = np.triu(corr)\n",
    "plot_cor_matrix(corr.round(2),mask,corr_label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(posts['post_like_normed'],bins=100,log=True,cumulative=True)\n",
    "plt.show()d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(posts['collect_normed'],bins=100,log=True,cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts = posts[(posts['1_tags']==0) & (posts['if_video']==1)]\n",
    "posts.index.name = 'index'\n",
    "posts.to_csv('./data/xhs/posts_images_tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv('./data/xhs/posts_images_tags.csv',index_col='index')\n",
    "dates = pd.DataFrame(posts['post_date'].value_counts()>=50).rename(\n",
    "    columns={'post_date':'period_50'})\n",
    "posts = posts.reset_index().set_index('post_date').join(dates)\n",
    "posts = posts[posts['period_50']==True].reset_index().set_index('index')\n",
    "posts = posts.rename(columns={'level_0':'post_date'})\n",
    "posts['post_date'] = pd.to_datetime(posts['post_date']).dt.to_period('d')\n",
    "posts['crawl_date'] = pd.to_datetime(posts['crawl_date']).dt.to_period('d')\n",
    "posts = posts[(posts['post_date']>='2022-03-20') &\\\n",
    "              (posts['post_date']<='2022-09-20')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axes = plt.subplots(1,1,figsize=(4,4))\n",
    "for n,col in enumerate(['post_date']):\n",
    "    X,Y = [],[]\n",
    "    for x,y in posts[[col]].value_counts().sort_index().items():\n",
    "        X.append(str(x[0]))\n",
    "        Y.append(y)\n",
    "    \n",
    "    axes[n].plot(X,Y)\n",
    "    axes[n].set_title(col)\n",
    "    axes[n].set_ylabel('No. of posts per day')\n",
    "    axes[n].set_xticks(np.arange(0,len(X),len(X)//10))\n",
    "    axes[n].set_xticklabels([X[i] for i in range(0,len(X),len(X)//10)],\n",
    "                            rotation=15,va='center',position=(0,-0.08))\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(,bins=100)\n",
    "plt.show()\n",
    "plt.hist(posts[['crawl_date']].value_counts().sort_index(),bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['post_date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = []\n",
    "all_edges = []\n",
    "keywords_by_post = {}\n",
    "\n",
    "for i,v in tqdm.tqdm(posts.iterrows()):\n",
    "    if v['keywords']==v['keywords']:\n",
    "        keywords = list(json.loads(v['keywords']).keys())\n",
    "        all_nodes+=keywords\n",
    "        all_edges+=[tuple(sorted(pair)) for pair in\\\n",
    "                    combinations(sorted(keywords),2)]\n",
    "        keywords_by_post[i]=keywords\n",
    "        \n",
    "all_edges = Counter(all_edges)\n",
    "all_edges = dict(sorted(all_edges.items(),\n",
    "                        key=lambda item:item[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_big_edges = {i:v for i,v in all_edges.items() if v>=100}\n",
    "print(len(all_edges))\n",
    "print(len(all_big_edges))\n",
    "\n",
    "G = nx.Graph()\n",
    "for pair,weight in tqdm.tqdm(all_edges.items()):\n",
    "    G.add_edge(pair[0],pair[1],weight=weight)\n",
    "\n",
    "H = nx.Graph()\n",
    "for pair,weight in tqdm.tqdm(all_big_edges.items()):\n",
    "    H.add_edge(pair[0],pair[1],weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comms_lp = nx_comm.label_propagation_communities(\n",
    "    H)\n",
    "comms_al = nx_comm.asyn_lpa_communities(\n",
    "    H,weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Modularity:',nx_comm.modularity(H,comms_gm))\n",
    "print('Modularity:',nx_comm.modularity(H,comms_lp))\n",
    "print('Modularity:',nx_comm.modularity(H,comms_al))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comms_gm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_103593/557736573.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomms_gm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcomms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcomms_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcomms_dict\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'comms_gm' is not defined"
     ]
    }
   ],
   "source": [
    "comms = comms_gm\n",
    "comms = [list(i) for i in comms]\n",
    "comms_dict = []\n",
    "for n,i in enumerate(comms):\n",
    "    comms_dict+=list(zip(i,[n]*len(i)))\n",
    "comms_dict = dict(comms_dict)    \n",
    "cmap_counts = len([len(i) for i in comms if len(i)>10])\n",
    "\n",
    "edges_same_comm = {i:[] for i in range(0,cmap_counts+1)}\n",
    "edges_spec_comm = []\n",
    "edges_diff_comm = []\n",
    "\n",
    "for (j,k) in tqdm.tqdm(all_big_edges):\n",
    "    \n",
    "    j_c = comms_dict[j]\n",
    "    k_c = comms_dict[k]\n",
    "    \n",
    "    if j_c==k_c:\n",
    "        if j_c<=cmap_counts:\n",
    "            edges_same_comm[j_c].append((j,k))\n",
    "        else:\n",
    "            edges_diff_comm.append((j,k))\n",
    "    else:\n",
    "        edges_spec_comm.append((j,k))\n",
    "\n",
    "edgewidth = [int(np.log(i[2]['weight'])) for i in H.edges(data=True)]\n",
    "edgewidth = [edge_-min(edgewidth)+1 for edge_ in edgewidth]\n",
    "pos = nx.spring_layout(H,k=.75,seed=100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,9),dpi=200)\n",
    "for n,comm in enumerate(comms):\n",
    "    if n<=cmap_counts:\n",
    "        rgba = cmap(n/cmap_counts)\n",
    "        color = matplotlib.colors.rgb2hex(rgba)\n",
    "    else:\n",
    "        color = 'grey'\n",
    "    nx.draw_networkx_nodes(H,pos,nodelist=comm,node_size=10,\n",
    "                           node_color=color,alpha=.75)\n",
    "\n",
    "for n,comm in edges_same_comm.items():\n",
    "    rgba = cmap(n/cmap_counts)\n",
    "    color = matplotlib.colors.rgb2hex(rgba)\n",
    "    nx.draw_networkx_edges(H,pos,edgelist=comm,width=edgewidth,\n",
    "                           edge_color=color,alpha=.25)\n",
    "nx.draw_networkx_edges(H,pos,edgelist=edges_spec_comm,width=edgewidth,\n",
    "                       edge_color='green',alpha=.25)    \n",
    "nx.draw_networkx_edges(H,pos,edgelist=edges_diff_comm,width=edgewidth,\n",
    "                       edge_color='k',alpha=.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in tqdm.tqdm(keywords_by_post.items()):\n",
    "    novelty = 0\n",
    "    for pair in [tuple(sorted(pair)) for pair in combinations(sorted(v),2)]:\n",
    "        if pair in edges_spec_comm:\n",
    "            novelty+=1\n",
    "    posts.loc[i,'novelty'] = novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.to_csv('./data/xhs/posts_novelty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "label_options = {\"ec\": \"k\", \"fc\": \"white\", \"alpha\":.70}\n",
    "nx.draw_networkx_labels(H, pos, font_size=10, bbox=label_options)\n",
    "font = {\"color\": \"k\", \"fontweight\": \"bold\", \"fontsize\": 14}\n",
    "ax.set_title(\"Topics as Nodes, Posts as Edges\", font)\n",
    "ax.set_xlim(-1.05,1.05)\n",
    "# ax.set_ylim(-1.15, 0.85)\n",
    "# ax.text(\n",
    "#     0.55,\n",
    "#     0.14,\n",
    "#     \"Edge width = log number \\nof posts w/o self-loops)\",\n",
    "#     horizontalalignment=\"left\",\n",
    "#     transform=ax.transAxes,\n",
    "#     fontdict=font)\n",
    "\n",
    "# ax.text(\n",
    "#     0.55,\n",
    "#     0.06,\n",
    "#     \"Node size = number of \\nposts w/ self-loops)\",\n",
    "#     horizontalalignment=\"left\",\n",
    "#     transform=ax.transAxes,\n",
    "#     fontdict=font)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = range(1,len(topics)+1)\n",
    "all_combi = {i:0 for i in combinations(iteration,2)}\n",
    "all_combi.update({(i,i):0 for i in iteration})\n",
    "topic_count = {i:0 for i in iteration}\n",
    "\n",
    "na = 0\n",
    "for i,v in tqdm.tqdm(posts.iterrows()):\n",
    "    topic_ = v['ai_pred_topics']\n",
    "\n",
    "    if topic_==topic_:\n",
    "        \n",
    "        if len(topic_)>=5:\n",
    "        \n",
    "            topic_ = [int(re.sub('[\\W_]+', '', s)) \\\n",
    "                      for s in topic_.split(',')]\n",
    "            \n",
    "            for i in topic_:\n",
    "                topic_count[i]+=1\n",
    "\n",
    "            if len(topic_)==1:\n",
    "                topic_ = [(topic_[0],topic_[0])]\n",
    "            else:\n",
    "                topic_ = list(combinations(sorted(topic_),2))\n",
    "\n",
    "            for top_ in topic_:\n",
    "                all_combi[top_]+=1\n",
    "        else:\n",
    "            na +=1\n",
    "    else:\n",
    "        na += 1\n",
    "        \n",
    "topic_count = {topics.loc[i]['name_en']:v for i,v in \\\n",
    "               topic_count.items() if v>0}\n",
    "topic_count = pd.DataFrame.from_dict(topic_count,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,1,figsize=(7,4))\n",
    "ax.bar(np.arange(len(topic_count[0].values)),\n",
    "        topic_count[0].values)\n",
    "ax.set_xticks(np.arange(len(topic_count[0].values)))\n",
    "ax.set_xticklabels(topic_count.index.values,rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum([v for i,v in all_combi.items()])\n",
    "print(total)\n",
    "all_combi_log = {i:int(np.log(v)) for i,v in all_combi.items() if v>0}\n",
    "all_combi_log = dict(sorted(all_combi_log.items(),\n",
    "                            key=lambda item:item[1],reverse=True))\n",
    "all_combi_log.pop((163,163))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = nx.MultiGraph()\n",
    "G = nx.Graph()\n",
    "for edge,weight in all_combi_log.items():\n",
    "    G.add_edge(edge[0], edge[1], weight=weight)\n",
    "labels = topics[['name_en']].to_dict()['name_en']\n",
    "G = nx.relabel_nodes(G,labels)\n",
    "\n",
    "pos = nx.spring_layout(G,k=.5,seed=100)\n",
    "edgewidth = [i[2]['weight'] for i in G.edges(data=True)]\n",
    "nodesize = [int(topic_count.loc[i,0]/30) for i in G.nodes()]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,9),dpi=200)\n",
    "nx.draw_networkx_edges(G, pos, alpha=.15, width=edgewidth, edge_color=\"k\")\n",
    "nx.draw_networkx_nodes(G, pos, node_size=nodesize, node_color=\"k\", alpha=.70)\n",
    "label_options = {\"ec\": \"k\", \"fc\": \"white\", \"alpha\":.70}\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, bbox=label_options)\n",
    "font = {\"color\": \"k\", \"fontweight\": \"bold\", \"fontsize\": 14}\n",
    "ax.set_title(\"Topics as Nodes, Posts as Edges\", font)\n",
    "ax.set_xlim(-1.05,1.05)\n",
    "# ax.set_ylim(-1.15, 0.85)\n",
    "# ax.text(\n",
    "#     0.55,\n",
    "#     0.14,\n",
    "#     \"Edge width = log number \\nof posts w/o self-loops)\",\n",
    "#     horizontalalignment=\"left\",\n",
    "#     transform=ax.transAxes,\n",
    "#     fontdict=font)\n",
    "\n",
    "# ax.text(\n",
    "#     0.55,\n",
    "#     0.06,\n",
    "#     \"Node size = number of \\nposts w/ self-loops)\",\n",
    "#     horizontalalignment=\"left\",\n",
    "#     transform=ax.transAxes,\n",
    "#     fontdict=font)\n",
    "\n",
    "plt.show()\n",
    "# nx.write_gexf(G,'./topics.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combi_weighted = {i:(total-v)/total for i,v in all_combi.items() if v>0}\n",
    "\n",
    "for i,v in tqdm.tqdm(posts.iterrows()):\n",
    "    topic_ = v['ai_pred_topics']\n",
    "\n",
    "    if topic_==topic_:\n",
    "        \n",
    "        if len(topic_)>=5:\n",
    "            topics_ = [int(re.sub('[\\W_]+', '', s)) for s in topic_.split(',')]\n",
    "            if len(topics_)==1:\n",
    "                topics_ = [(topics_[0],topics_[0])]\n",
    "            else:\n",
    "                topics_ = list(combinations(sorted(topics_),2))\n",
    "            scores_ = []\n",
    "            for i_ in topics_:\n",
    "                scores_.append(all_combi_weighted[i_])\n",
    "            posts.loc[i,'creativity'] = np.mean(scores_)\n",
    "        else:\n",
    "            posts.loc[i,'creativity'] = np.NaN\n",
    "    else:\n",
    "        posts.loc[i,'creativity'] = np.NaN            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combi_weighted = {i:(total-v)/total for i,v in all_combi.items() if v>0}\n",
    "\n",
    "for i,v in tqdm.tqdm(posts.iterrows()):\n",
    "    topic_ = v['ai_pred_topics']\n",
    "\n",
    "    if topic_==topic_:\n",
    "        \n",
    "        if len(topic_)>=5:\n",
    "            topics_ = [int(re.sub('[\\W_]+', '', s)) for s in topic_.split(',')]\n",
    "            if len(topics_)==1:\n",
    "                topics_ = [(topics_[0],topics_[0])]\n",
    "            else:\n",
    "                topics_ = list(combinations(sorted(topics_),2))\n",
    "            scores_ = []\n",
    "            posts.loc[i,'creativity_1'] = len(topics_)            \n",
    "        else:\n",
    "            posts.loc[i,'creativity_1'] = np.NaN\n",
    "    else:\n",
    "        posts.loc[i,'creativity_1'] = np.NaN            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['creativity_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video_env_1",
   "language": "python",
   "name": "video_env_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
